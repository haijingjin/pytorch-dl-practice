{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t\n",
    "This notebook contains code and concepts from the \"Introduction to Deep Learning with PyTorch\" course on DataCamp, originally developed by Maham Khan.\n",
    "\n",
    "I have added extra comments, explanations, and modifications to the code to aid understanding and provide additional context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0108, -0.0725]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "## Create input\n",
    "# tensor with three features\n",
    "input_tensor = torch.tensor(\n",
    "[[0.3471, 0.4547, -0.2356]])\n",
    "\n",
    "# Define our first linear layer\n",
    "linear_layer = nn.Linear(in_features=3, out_features=2)\n",
    "\n",
    "# Pass input through linear layer\n",
    "output = linear_layer(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_layer.weight:  Parameter containing:\n",
      "tensor([[ 0.4233,  0.1631,  0.3090],\n",
      "        [-0.4805, -0.2379,  0.3753]], requires_grad=True)\n",
      "linear_layer.bias:  Parameter containing:\n",
      "tensor([-0.1591,  0.2909], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Print the weight and bias of the linear layer\n",
    "print(\"linear_layer.weight: \", linear_layer.weight)\n",
    "print(\"linear_layer.bias: \", linear_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer operation \n",
    "\n",
    "For input $X$, with weights $W_0$ and bias $b_0$, the linear layer operation is:\n",
    "\n",
    "$y_0 = W_0 * X + b_0$\n",
    "\n",
    "- weights $W_0$ and bias $b_0$ are randomly initialized \n",
    "- $y_0$ is the output of the linear layer\n",
    "- tuning the weights and biases is the process of training the model\n",
    "- weights and biases are adjusted to minimize the loss function\n",
    "- the loss function measures how well the model's predictions match the actual data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0108, -0.0725]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Pass input through linear layer again\n",
    "output = linear_layer(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1432, -0.1754, -0.0240,  0.7588,  0.3835], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Random values from normal distribution (mean=0, std=1)\n",
    "input_tensor = torch.randn(10) \n",
    "\n",
    "# Create network with three linear layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 18),\n",
    "    nn.Linear(18, 20),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "\n",
    "# Pass input through model\n",
    "output = model(input_tensor)\n",
    "print(output)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.4837e-01,  5.4804e-02, -7.4907e-01, -3.8952e-01,  5.0334e-01,\n",
       "          1.0518e+00,  4.2172e-01, -2.3033e+00, -9.0051e-02, -1.0382e-01],\n",
       "        [-1.3934e+00, -1.4990e+00, -6.9818e-01, -7.0174e-01, -9.5970e-01,\n",
       "         -5.4904e-02, -5.1799e-01, -2.0767e-01, -1.3028e+00,  5.2704e-01],\n",
       "        [-9.9970e-01, -1.3610e+00, -1.6454e+00,  3.7918e-01, -1.9644e+00,\n",
       "          5.0976e-01,  1.4878e+00,  8.3231e-01,  3.4670e-01, -1.6091e+00],\n",
       "        [ 9.5728e-01,  8.4726e-01,  3.5618e-01, -5.4742e-01,  7.2314e-01,\n",
       "         -1.1938e+00,  1.2719e-01, -3.6542e-01,  1.2577e-01,  1.0746e+00],\n",
       "        [ 1.0326e-01,  9.1493e-01, -5.1294e-01,  1.2244e+00, -6.2411e-01,\n",
       "          3.5623e-01,  1.8922e-03, -6.4831e-01, -3.8082e-01,  3.3649e-01]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch of 32 samples, each with 10 features\n",
    "batch_tensor = torch.randn(32, 10)  # Shape: (batch_size, features)\n",
    "\n",
    "# Shape of batch tensor\n",
    "batch_tensor.shape\n",
    "\n",
    "# First 5 samples in the batch\n",
    "batch_tensor[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need activation functions?\n",
    "\n",
    "- activation functions introduce nonlinearity into the model\n",
    "- without activation functions, the model would be a linear regression\n",
    "- activation functions allow the model to learn more complex relationships\n",
    "\n",
    "#### Sigmoid function\n",
    "- sigmoid function is a nonlinear function that maps any input value to a value between 0 and 1\n",
    "- it is defined as $f(x) = 1 / (1 + e^{-x})$\n",
    "- it is commonly used as the activation function for the last layer of a binary classification model    \n",
    "\n",
    "\n",
    "#### Softmax function\n",
    "- softmax function is a nonlinear function that maps any input value to a value between 0 and 1\n",
    "- it is defined as $f(x) = e^x / \\sum_{i=1}^{n} e^{x_i}$\n",
    "- it is commonly used as the activation function for the last layer of a multiclass classification model\n",
    "- the output of the softmax function is a probability distribution over the classes and the sum of the probabilities is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9975]])\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid function\n",
    "input_tensor = torch.tensor([[6.0]])\n",
    "sigmoid = nn.Sigmoid()\n",
    "output = sigmoid(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5467], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid function is equivalent to logistic regression when it is used as the last layer of a neural network\n",
    "\n",
    "input_tensor = torch.randn(6) \n",
    "\n",
    "# Create network with two linear layers and a sigmoid activation function\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6, 4), # First linear layer\n",
    "    nn.Linear(4, 1), # Second linear layer\n",
    "    nn.Sigmoid() # Sigmoid activation function\n",
    ")\n",
    "\n",
    "# Pass input through model\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.]])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.tensor([[6.0]])\n",
    "print(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1392, 0.8420, 0.0188]])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Create an input tensor\n",
    "# 3 features\n",
    "# double brackets to create a 2D tensor with shape (1, 3)\n",
    "# think of it as a batch of 1 sample with 3 features\n",
    "input_tensor = torch.tensor([[4.3, 6.1, 2.3]])\n",
    "# Apply softmax along the last dimension, the features \n",
    "# dim=-1 means the softmax function is applied to each row of the input tensor\n",
    "probabilities = nn.Softmax(dim=-1)\n",
    "output_tensor = probabilities(input_tensor)\n",
    "print(output_tensor)\n",
    "print(torch.sum(output_tensor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 7, 9])\n",
      "tensor([ 6, 15])\n",
      "tensor([ 6, 15])\n",
      "tensor(21)\n"
     ]
    }
   ],
   "source": [
    "# more on torch dimensions\n",
    "\n",
    "# Create a 2D tensor\n",
    "x = torch.tensor([[1, 2, 3],\n",
    "                 [4, 5, 6]])\n",
    "# Shape is (2, 3): 2 rows, 3 columns\n",
    "\n",
    "print(torch.sum(x, dim=0))  # Sums down columns: [5, 7, 9]\n",
    "print(torch.sum(x, dim=1))  # Sums across rows: [6, 15]\n",
    "print(torch.sum(x, dim=-1)) # Same as dim=1: [6, 15]\n",
    "print(torch.sum(x)) # Sum all elements: 21"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
