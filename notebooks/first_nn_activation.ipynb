{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t\n",
    "This notebook contains code and concepts from the \"Introduction to Deep Learning with PyTorch\" course on DataCamp, originally developed by Maham Khan.\n",
    "\n",
    "I have added extra comments, explanations, and modifications to the code to aid understanding and provide additional context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6192,  0.0019]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "## Create input\n",
    "# tensor with three features\n",
    "input_tensor = torch.tensor(\n",
    "[[0.3471, 0.4547, -0.2356]])\n",
    "\n",
    "# Define our first linear layer\n",
    "linear_layer = nn.Linear(in_features=3, out_features=2)\n",
    "\n",
    "# Pass input through linear layer\n",
    "output = linear_layer(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_layer.weight:  Parameter containing:\n",
      "tensor([[ 0.1637, -0.4926, -0.2364],\n",
      "        [ 0.2856, -0.4685, -0.3842]], requires_grad=True)\n",
      "linear_layer.bias:  Parameter containing:\n",
      "tensor([-0.5078,  0.0252], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Print the weight and bias of the linear layer\n",
    "print(\"linear_layer.weight: \", linear_layer.weight)\n",
    "print(\"linear_layer.bias: \", linear_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer operation \n",
    "\n",
    "For input $X$, with weights $W_0$ and bias $b_0$, the linear layer operation is:\n",
    "\n",
    "$y_0 = W_0 * X + b_0$\n",
    "\n",
    "- weights $W_0$ and bias $b_0$ are randomly initialized \n",
    "- $y_0$ is the output of the linear layer\n",
    "- tuning the weights and biases is the process of training the model\n",
    "- weights and biases are adjusted to minimize the loss function\n",
    "- the loss function measures how well the model's predictions match the actual data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass input through linear layer again\n",
    "output = linear_layer(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1147,  0.1996, -0.4057,  0.1150, -0.1326], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Random values from normal distribution (mean=0, std=1)\n",
    "input_tensor = torch.randn(10) \n",
    "\n",
    "# Create network with three linear layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 18),\n",
    "    nn.Linear(18, 20),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "\n",
    "# Pass input through model\n",
    "output = model(input_tensor)\n",
    "print(output)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8262, -1.5991, -0.5800,  1.0755,  0.2274,  1.1292, -0.2907,  1.3053,\n",
       "          1.3232,  0.6945],\n",
       "        [ 1.5327, -0.4081,  0.1420, -1.3438,  1.7942,  0.4449, -1.1740, -0.0416,\n",
       "          0.7742,  0.5491],\n",
       "        [ 0.3964,  0.7389,  0.4995, -1.7049, -1.7698,  0.2849, -0.1710, -1.2045,\n",
       "         -1.2469, -0.1376],\n",
       "        [-0.4337, -0.0896,  0.0887, -0.3950, -0.3887, -1.7668,  1.2882, -0.8495,\n",
       "          0.7437, -0.7374],\n",
       "        [-0.3934,  1.6661,  0.1323, -1.9737,  2.4621, -0.8096,  1.9039,  1.2751,\n",
       "         -0.8228,  0.1221]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch of 32 samples, each with 10 features\n",
    "batch_tensor = torch.randn(32, 10)  # Shape: (batch_size, features)\n",
    "\n",
    "# Shape of batch tensor\n",
    "batch_tensor.shape\n",
    "\n",
    "# First 5 samples in the batch\n",
    "batch_tensor[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "# Sigmoid function is equivalent to logistic regression when it is used as the last layer of a neural network\n",
    "input_tensor = torch.tensor([[6.0]])\n",
    "sigmoid = nn.Sigmoid()\n",
    "output = sigmoid(input_tensor)\n",
    "print(output)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6, 4), # First linear layer\n",
    "    nn.Linear(4, 1), # Second linear layer\n",
    "    nn.Sigmoid() # Sigmoid activation function\n",
    ")\n",
    "\n",
    "# Pass input through model\n",
    "output = model(input_tensor)\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
